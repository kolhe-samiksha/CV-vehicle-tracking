{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f8579-c7b5-420e-aff2-ffe6716edc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "os.system('pip install tensorflow')\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3a5cee-194e-4174-9898-fe7e626229d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cap= cv2.VideoCapture(\"tvid_Trim.mp4\")\n",
    "\n",
    "path='/home/katonic/yolov5vehiclecount/yolov5s-int8.tflite'\n",
    "#count=0\n",
    "\n",
    "model = torch.hub.load('/home/katonic/yolov5', 'custom',path,source='local')\n",
    "\n",
    "#model = 'Vehicle-Counting/yolov5s.pt'\n",
    "#model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "\n",
    "b=model.names[2] = 'car'\n",
    "size=416\n",
    "count=0\n",
    "counter=0\n",
    "color=(0,0,255)\n",
    "cy1=250\n",
    "offset=6\n",
    "while True:\n",
    "    ret,img=cap.read()\n",
    "    count += 1\n",
    "    if count % 4 != 0:\n",
    "        continue\n",
    "    if isinstance(img, type(None)):\n",
    "        break   \n",
    "    img = cv2.resize(img,(640,640))\n",
    "    #img = img[np.newaxis,:, :, :]\n",
    "    #img = tf.expand_dims(img,0)\n",
    "    #img = torch.from_numpy(img)\n",
    "    cv2.line(img,(79,cy1),(599,cy1),(0,0,255),2)\n",
    "    results = model(img,size)\n",
    "    a = results.pandas().xyxy[0]\n",
    "    for index,row in results.pandas().xyxy[0].iterrows():\n",
    "        x1 = int(row['xmin'])\n",
    "        y1 = int(row['ymin'])\n",
    "        x2 = int(row['xmax'])\n",
    "        y2 = int(row['ymax'])\n",
    "        d = (row['class'])\n",
    "        if d==2:\n",
    "            cv2.rectangle(img,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "            rectx1,recty1 = ((x1+x2)/2,(y1+y2)/2)\n",
    "            rectcenter = float(rectx1)\n",
    "            rectcenter = float(recty1)\n",
    "            cx = rectcenter[0]\n",
    "            cy = rectcenter[1]\n",
    "            cv2.circle(img,(cx,cy),3,(0,255,0),-1)\n",
    "            cv2.putText(img,str(b),(x1,y1),cv2.FONT_HERSHEY_PLAIN,2,(255,255,255),2)\n",
    "            if cy<(cy1+offset) and cy>(cy1-offset):\n",
    "                counter+=1\n",
    "                cv2.line(img,(79,cy1),(599,cy1),(0,0,255),2)\n",
    "                cv2.putText(img,str(counter),(x2,y2),cv2.FONT_HERSHEY_PLAIN,2,(255,0,0),2)\n",
    "               \n",
    "    if cv2.waitKey(1) & 0xFF==27:\n",
    "        break\n",
    "         \n",
    "cap.release()\n",
    "#cv2.destroyAllWindows()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ba98f68-ddb3-4e79-b1a4-cf80eb5cc353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v7.0-108-g4db6757 Python-3.8.15 torch-1.13.1+cpu CPU\n",
      "\n",
      "Loading /home/katonic/yolov5vehiclecount/yolov5s-int8.tflite for TensorFlow Lite inference...\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "cap= cv2.VideoCapture(\"tvid_Trim.mp4\")\n",
    "\n",
    "path='/home/katonic/yolov5vehiclecount/yolov5s-int8.tflite'\n",
    "model = torch.hub.load('/home/katonic/yolov5', 'custom',path,source='local')\n",
    "\n",
    "b=model.names[2] = 'car'\n",
    "size=416\n",
    "count=0\n",
    "counter=0\n",
    "color=(0,0,255)\n",
    "cy1=250\n",
    "offset=6\n",
    "cap= cv2.VideoCapture(\"tvid_Trim.mp4\")\n",
    "ret,img=cap.read()\n",
    "while True:\n",
    "    ret,img=cap.read()\n",
    "    count += 1\n",
    "    if count % 4 != 0:\n",
    "        continue\n",
    "    if isinstance(img, type(None)):\n",
    "        break   \n",
    "    img = cv2.resize(img,(640,640))\n",
    "    #img = img[np.newaxis,:,:,:]\n",
    "    cv2.line(img,(79,cy1),(599,cy1),(0,0,255),2)\n",
    "    results = model(img,size)\n",
    "    a = results.pandas().xyxy[0]\n",
    "    for index,row in results.pandas().xyxy[0].iterrows():\n",
    "        x1 = int(row['xmin'])\n",
    "        y1 = int(row['ymin'])\n",
    "        x2 = int(row['xmax'])\n",
    "        y2 = int(row['ymax'])\n",
    "        d = (row['class'])\n",
    "    if d==2:\n",
    "        cv2.rectangle(img,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "        rectx1,recty1 = ((x1+x2)/2,(y1+y2)/2)\n",
    "        rectcenter = (int(rectx1),int(recty1))\n",
    "        cx = rectcenter[0]\n",
    "        cy = rectcenter[1]\n",
    "        cv2.circle(img,(cx,cy),3,(0,255,0),-1)\n",
    "        cv2.putText(img,str(b),(x1,y1),cv2.FONT_HERSHEY_PLAIN,2,(255,255,255),2)\n",
    "        if cy<(cy1+offset) and cy>(cy1-offset):\n",
    "                counter+=1\n",
    "                cv2.line(img,(79,cy1),(599,cy1),(0,0,255),2)\n",
    "                cv2.putText(img,str(counter),(x2,y2),cv2.FONT_HERSHEY_PLAIN,2,(255,0,0),2)\n",
    "                \n",
    "        cv2.imwrite(f'/home/katonic/yolov5vehiclecount/output1/result_bb_{count}.png',img)\n",
    "        \n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfbecae-4326-4b67-bac7-00c5f3317027",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "361b0591-95bd-4f53-973e-f5bfea26c691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 7, 0, 2, 5, 5, 5, 2, 2, 6, 0, 7, 5, 2, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = results.pandas().xyxy[0]\n",
    "res = []\n",
    "for index,row in results.pandas().xyxy[0].iterrows():\n",
    "    class_n = (row['class'])\n",
    "    res.append(class_n)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6566548a-d69c-400c-b651-b775fce88a5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'metrics' from 'pytorch_lightning' (/opt/conda/lib/python3.8/site-packages/pytorch_lightning/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n\u001b[1;32m     10\u001b[0m SMOOTH \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mdef iou_numpy(outputs: np.array, labels: np.array):   #input/output bounding_box_co-ordinates\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    outputs = outputs.squeeze(1)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    return thresholded\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'metrics' from 'pytorch_lightning' (/opt/conda/lib/python3.8/site-packages/pytorch_lightning/__init__.py)"
     ]
    }
   ],
   "source": [
    "#IOU Metrics - for object localisation in bounding boxes & \n",
    "import numpy as np\n",
    "import math\n",
    "import torchmetrics\n",
    "from torchmetrics import ConfusionMatrix\n",
    "import torch\n",
    "import pytorch_lightning\n",
    "from pytorch_lightning import metrics\n",
    "\n",
    "SMOOTH = 1e-6\n",
    "'''\n",
    "def iou_numpy(outputs: np.array, labels: np.array):   #input/output bounding_box_co-ordinates\n",
    "    outputs = outputs.squeeze(1)\n",
    "\n",
    "    intersection = (outputs & labels).sum((1, 2))\n",
    "    union = (outputs | labels).sum((1, 2))\n",
    "\n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)\n",
    "\n",
    "    thresholded = np.ceil(np.clip(20 * (iou - 0.7), 0, 10)) / 10\n",
    "\n",
    "    return thresholded\n",
    "'''\n",
    "'''\n",
    "def iou(x1,y1,x2,y2):\n",
    "    interArea = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n",
    "    width_box1 = \n",
    "    height_box1 = \n",
    "    width_box2 = \n",
    "    height_box2 = \n",
    "    box1_area = width_box1 * height_box1\n",
    "    box2_area = width_box2 * height_box2\n",
    "    area_union = box1_area + box2_area - interArea\n",
    "    iou = area_union / interArea\n",
    "    return iou\n",
    "'''\n",
    "preds = [0] * 200 + [1] * 30 + [0] * 10 + [1] * 20\n",
    "targets = [0] * 200 + [1] * 30 + [1] * 10 + [0] * 20\n",
    "\n",
    "preds = torch.tensor(preds)\n",
    "targets = torch.tensor(targets)\n",
    "\n",
    "def _print_some_metrics(preds, targets, num_classes):\n",
    "    precision = metrics.classification.Precision(num_classes=num_classes)\n",
    "    recall = metrics.classification.Recall(num_classes=num_classes)\n",
    "    f1 = metrics.classification.F1(num_classes=num_classes)\n",
    "\n",
    "    accuracy = metrics.classification.Accuracy()\n",
    "    avg_precision = metrics.classification.AveragePrecision(\n",
    "        num_classes=1)\n",
    "    \n",
    "    print(\"Precision:\\n{}\\n\".format(precision(preds, targets)))\n",
    "    print(\"Recall:\\n{}\\n\".format(recall(preds, targets)))\n",
    "    print(\"F1:\\n{}\\n\".format(f1(preds, targets)))\n",
    "\n",
    "    print(\"AVG Precision:\\n{}\\n\".format(avg_precision(preds, targets)))\n",
    "    print(\"Accuracy:\\n{}\\n\".format(accuracy(preds, targets)))\n",
    "    print(\"ConfMat:\\n{}\\n\".format(confusion_matrix(preds, targets)))\n",
    "\n",
    "\n",
    "pred_results = _print_some_metrics(preds, targets, num_classes=1)\n",
    "print(pred_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2a0395-72c8-4f68-833b-4895faac5b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import torchmetrics\n",
    "from torchmetrics import ConfusionMatrix\n",
    "\n",
    "\n",
    "confmat = ConfusionMatrix(num_classes=1)\n",
    "confmat(preds, target)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cd8645-0a60-45f1-8a14-bfca3f45bd11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b12d798-2109-4468-9bb5-58762989d177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
